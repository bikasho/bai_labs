{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEOROVbCdXIc"
   },
   "source": [
    "# Smoker Detector AI application\n",
    "\n",
    "\n",
    "\n",
    "Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, you might want to detect cigarate smoker using a smart AI camera. In this project, we will expolre deep learning to identify a smoker in a video stream. To do this, we first build our base model. We will use a pretrained imageNet model and replace ImageNet last layer with a logistic classifier ( binary classifier ) as our overall application architecture.\n",
    "\n",
    "**Goal** : Design and Build a Deep Learning Model to identify a cigarret smoker in a video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8JUvQizdXId"
   },
   "source": [
    "### Base Model\n",
    "\n",
    "The project (base model) is broken down into multiple steps:\n",
    "\n",
    "* Load and preprocess the videos dataset if required\n",
    "* Build a logistic classifier\n",
    "* Train the classifier on our video dataset\n",
    "* Use the trained classifier to predict\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "![](https://drive.google.com/open?id=1TwPtHmekcdGqQeOgF3Rb_EFB1X6QIsOe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGwaOc8jdXIe"
   },
   "source": [
    "**TO DO**\n",
    "\n",
    "1. What input data is required? How you can represent input data i.e videos for imageNet model to consume?\n",
    "\n",
    "  ANS 1. Input given to ImageNet would be frames of video. We could use the same code used in face_net to achieve this.\n",
    "\n",
    "\n",
    "2. What should be an ideal video resolution?\n",
    "\n",
    "  ANS 2. An ideal video resolution should be decided on the basis of, if a human can identify if there's a smoking instance in the clip with ease, that determines the ideal video resolution and anything worse than that shall be discarded from the data to be fed to the model.\n",
    "\n",
    "3. What is the output?\n",
    "\n",
    "  ANS 3. Output must be a '0' for non-smoking clip or '1' for smoking clip at the end of the final classifier layer.\n",
    "\n",
    "\n",
    "4. What Architecture? ( with respect to the above diagram? )\n",
    "\n",
    "  ANS 4. We are going to try Alexnet and Resnet and compare both to choose the better model. \n",
    "\n",
    "5. What loss function to use for this model?\n",
    "\n",
    "  ANS 5. Logistic loss (binary cross-entropy) is the loss function to use for a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMcsys7PdXIf"
   },
   "source": [
    "First up is importing the packages you'll need. It's good practice to keep all the imports at the beginning of your code. As you work through this notebook and find you need to import a package, make sure to add the import up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16600,
     "status": "ok",
     "timestamp": 1588847184673,
     "user": {
      "displayName": "Anurag Patro",
      "photoUrl": "",
      "userId": "11440151903611155456"
     },
     "user_tz": -330
    },
    "id": "8SmBKA2-dXIg",
    "outputId": "33aa0ace-97af-4c68-ccf2-43d40cb2bc7d"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json, time, copy\n",
    "import cv2,os, glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kf3j7fFldXIm"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Here you'll use `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). You'll also need to make sure the input data is resized to 224x224??? pixels as required by the pre-trained networks.\n",
    "\n",
    "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this you don't want any scaling or rotation transformations, but you'll need to resize then crop the images to the appropriate size.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5U4tJ70EnaK"
   },
   "source": [
    "We will first extract frames out of the given video. For the time being will add a label 1 ( True ) for each frame of the video if it is from smoking folder and 0 ( False ) otherwise. In the below function, read and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbdk8HsvbYg4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anuragpatro/SmokeDetectionAI\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "path = '/home/anuragpatro/SmokeDetectionAI/Videos'\n",
    "data_path = '/home/anuragpatro/SmokeDetectionAI/data'\n",
    "def read_frame(classname,label):\n",
    "  vid = 0\n",
    "  for filename in os.listdir(os.path.join(path,classname)):\n",
    "    vid +=1\n",
    "    print(\"%d \"%vid,end = \" \")\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(os.path.join(path,classname,filename))   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    x=1\n",
    "    while(cap.isOpened()):\n",
    "      frameId = cap.get(1) #current frame number\n",
    "      ret, frame = cap.read()\n",
    "      if(ret != True):\n",
    "        break\n",
    "      imgname =\"%d%dframe%d.jpg\" %(label,vid,count);count+=1\n",
    "      cv2.imwrite(os.path.join(data_path,imgname), frame)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60896,
     "status": "ok",
     "timestamp": 1588763846674,
     "user": {
      "displayName": "Anurag Patro",
      "photoUrl": "",
      "userId": "11440151903611155456"
     },
     "user_tz": -330
    },
    "id": "E4vEiR7AckJK",
    "outputId": "bab8fdc4-8957-4581-cdf3-745c5a832f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  "
     ]
    }
   ],
   "source": [
    "read_frame('Smoking_dataset',1)\n",
    "read_frame('NonSmoking_dataset',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining transforms\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/anuragpatro/SmokeDetectionAI/data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "dirs = {'train': train_dir, \n",
    "        'valid': valid_dir, \n",
    "        'test' : test_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(dirs[x],transform=data_transform[x]) for x in ['train', 'valid', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['train', 'valid', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) \n",
    "                              for x in ['train', 'valid', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 36405, 'valid': 7801, 'test': 7801}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['non_smoke', 'smoke']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_sizes)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWqR9eLCdXIx"
   },
   "source": [
    "### Label mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQdL0t_idXI3"
   },
   "source": [
    "# Building and training the classifier\n",
    "\n",
    "Now that the data is ready, it's time to build and train the classifier. As usual, you should use one of the pretrained models from `torchvision.models` to get the image features. Build and train a new feed-forward classifier using those features.\n",
    "\n",
    "Things you'll need to do:\n",
    "\n",
    "* Load a [pre-trained network](http://pytorch.org/docs/master/torchvision/models.html) (If you need a starting point, the VGG networks work great and are straightforward to use)\n",
    "* Define a new, untrained feed-forward network as a classifier ( Logistic Classifier ), using ReLU activations and dropout\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set to determine the best hyperparameters\n",
    "\n",
    "\n",
    "When training make sure you're updating only the weights of the feed-forward network. You should be able to get the validation accuracy above 70% if you build everything right. Make sure to try different hyperparameters (learning rate, units in the classifier, epochs, etc) to find the best model. Save those hyperparameters to use as default values in the next part of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.alexnet(pretrained = True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(9216, 4096)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(4096, len(class_names))),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=4096, out_features=2, bias=True)\n",
       "    (output): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = classifier\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler,    \n",
    "                                      num_epochs=10, device='cuda'):\n",
    "    since = time.time()\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criteria NLLLoss which is recommended with Softmax final layer\n",
    "criteria = nn.NLLLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "# Decay LR by a factor of 0.1 every 4 epochs\n",
    "sched = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "# Number of epochs\n",
    "eps=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyWsES3PdXI9"
   },
   "source": [
    "### Training the model on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuragpatro/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1827 Acc: 0.9450\n",
      "valid Loss: 0.0369 Acc: 0.9886\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "train Loss: 0.0839 Acc: 0.9694\n",
      "valid Loss: 0.0252 Acc: 0.9919\n",
      "\n",
      "Training complete in 37m 6s\n",
      "Best val Acc: 0.991924\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criteria, optimizer, sched, eps, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARV6dF6MdXJB"
   },
   "source": [
    "## Testing your network\n",
    "\n",
    "It's good practice to test your trained network on test data, images/videos the network has never seen either in training or validation. This will give you a good estimate for the model's performance on completely new videos. Run the test videos through the network and measure the accuracy, the same way you did validation. You should be able to reach around 70% accuracy on the test set if the model has been trained well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XlKUfYaBdXJC"
   },
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "def calc_accuracy(model, data, cuda=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    model.to(device='cuda')    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloaders[data]):\n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # obtain the outputs from the model\n",
    "            outputs = model.forward(inputs)\n",
    "            # max provides the (maximum probability, max value)\n",
    "            _, predicted = outputs.max(dim=1)\n",
    "            # check the \n",
    "            if idx == 0:\n",
    "                print(predicted) #the predicted class\n",
    "                print(torch.exp(_)) # the predicted probability\n",
    "            equals = predicted == labels.data\n",
    "            correct += int(torch.sum(equals == True))\n",
    "            total += len(equals)\n",
    "            if idx == 0:\n",
    "                print(equals)\n",
    "    print(\"Final Test Accuracy : \"+str(round(correct/total,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_accuracy(model_ft,'test', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZCYGMTYdXJF"
   },
   "source": [
    "## Save the checkpoint\n",
    "\n",
    "Now that your network is trained, save the model so you can load it later for making predictions. \n",
    "\n",
    "Remember that you'll want to completely rebuild the model later so you can use it for inference. Make sure to include any information you need in the checkpoint. If you want to load the model and keep training, you'll want to save the number of epochs as well as the optimizer state, `optimizer.state_dict`. You'll likely want to use this trained model in the next part of the project, so best to save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JyfP-ZcbdXJG"
   },
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint \n",
    "state = {'arch': 'alex',\n",
    "            'state_dict': model.state_dict()}\n",
    "torch.save(state,'alexnetsave.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VQm4fHvdXJJ"
   },
   "source": [
    "## Loading the checkpoint\n",
    "\n",
    "At this point it's good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ddzd6ycSdXJK"
   },
   "outputs": [],
   "source": [
    "#Loading checkpoint\n",
    "model = torch.load('alexnetsave.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEOebw5VdXJP"
   },
   "source": [
    "# Inference for classification\n",
    "\n",
    "Now you'll write a function to use a trained network for inference. That is, you'll pass an image into the network and predict the class. Write a function called `predict` that takes an video and a model, \n",
    "\n",
    "First you'll need to handle processing the input image such that it can be used in your network. \n",
    "\n",
    "## Video Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CQ6MCLZMdXJQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Process a PIL image for use in a PyTorch model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7VVAR5lMdXJT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of smoker_detector_base.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ppujari/bai_labs/blob/master/smoker_detector_base.ipynb",
     "timestamp": 1588593589483
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
